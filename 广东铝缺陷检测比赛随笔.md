# 广东铝缺陷检测比赛

比赛介绍：

​	https://tianchi.aliyun.com/competition/introduction.htm?spm=5176.100066.0.0.1b4733afBuhOKM&raceId=231682

比赛数据：





解决方案/解题思路：

根据比赛的相关介绍和对比赛数据的初步观察，将其定位为图像处理中的==单标签的多分类问题==，因此大方向上采用分类模型去解答，目前较为主流的分类模型有resnet，inception v4， densenet，为此，打算从这几个网络着手！

1）将train数据进行切分为训练数据和验证数据：

比赛给了两个部分的数据，一部分是有标签的训练数据，还有一部分是没有标签的测试数据，为了更快的算法迭代以及验证模型效果，需要先将有标签的训练数据进行切分，切为两个部分：train与val数据，采用sklearn中的sklearn.model_selection模块下的train_test_split方法，该数据集是根据文件夹的名字来标注类别的，因此首先遍历文件得到相应的label，参考其他人的代码（https://github.com/Herbert95/tianchi_lvcai/blob/master/gen_label_csv.py），得到图片的路径以及对应的label，代码如下：

```python
  1 import os
  2 import math
  3 import numpy as np
  4 import pandas as pd
  5 import os.path as osp
  6 
  7 label_warp = {'正常': 0,
  8               '不导电': 1,
  9               '擦花': 2,
 10               '横条压凹': 3,
 11               '桔皮': 4,
 12               '漏底': 5,
 13               '碰伤': 6,
 14               '起坑': 7,
 15               '凸粉': 8,
 16               '涂层开裂': 9,
 17               '脏点': 10,
 18               '其他': 11,
 19               }
 20 
 21 # train data
 22 data_path = 'data/guangdong_round1_train2_20180916'
 23 img_path, label = [], []
 24 
 25 print(os.listdir(data_path))
 26 for first_path in os.listdir(data_path):
 27     if first_path == '.DS_Store':
 28         continue
 29     first_path = osp.join(data_path, first_path)
 30     if '无瑕疵样本' in first_path:
 31         for img in os.listdir(first_path):
 32             img_path.append(osp.join(first_path, img))
 33             label.append('正常')
 34     else:
 35         for second_path in os.listdir(first_path):
 36             if second_path == '.DS_Store':
 37                 continue
 38             defect_label = second_path
 39             second_path = osp.join(first_path, second_path)
 40             if defect_label != '其他':
 41                 for img in os.listdir(second_path):
 42                     img_path.append(osp.join(second_path, img))
 43                     label.append(defect_label)
 44             else:
 45                 for third_path in os.listdir(second_path):
 46                     third_path = osp.join(second_path, third_path)
 47                     if osp.isdir(third_path):
 48                         for img in os.listdir(third_path):
 49                             if 'DS_Store' not in img:
 50                                 img_path.append(osp.join(third_path, img))
 51                                 label.append(defect_label)
 52 
 53 label_file = pd.DataFrame({'img_path': img_path, 'label': label})
 54 label_file['label'] = label_file['label'].map(label_warp)
 55 
 56 label_file.to_csv('data/label.csv', index=False)
 57 
 58 # test data
 59 test_data_path = 'data/guangdong_round1_test_a_20180916'
 60 all_test_img = os.listdir(test_data_path)
 61 test_img_path = []
 62 
 63 for img in all_test_img:
 64     if osp.splitext(img)[1] == '.jpg':
 65         test_img_path.append(osp.join(test_data_path, img))
 66 
 67 test_file = pd.DataFrame({'img_path': test_img_path})
 68 test_file.to_csv('data/test.csv', index=False)
```

得到了文件的路径以及对应的文件类别，分析数据特征，画出相应的统计图：

统计类别：

![guangdong_bar](/Users/majian/Documents/machine learning/广东铝缺陷检测比赛/data/guangdong_bar.png)



![guangdong_pie](/Users/majian/Documents/machine learning/广东铝缺陷检测比赛/data/guangdong_pie.png)

从统计图上可以得出如下结论：

1. 类别极为不均衡，其中norm的占比为47.7%，因此后续对于样本的采样需要进行根据类别进行分层采样

2. 有些类别的数据量只有几十张，后续需要考虑数据增强



采用sklearn中train_test_split方法来切割训练与验证集，由于本数据集的类别不均衡，因此需要采用==分层抽样==，代码如下：

```python
  1 import pandas as pd
  2 import numpy as np
  3 from sklearn.model_selection import train_test_split
  4 
  5 # 读取训练图片列表
  6 all_data = pd.read_csv('data/label.csv')
  7 print('len of all_data:',len(all_data))
  8 # 分离训练集和测试集，stratify参数用于分层抽样
  9 train_data_list, val_data_list = train_test_split(all_data, test_size=0.3, random_state=666, stratify=all_data['label'])
 10 
 11 print(train_data_list)
```

2）构建网络：densenet



代码如下：

```python
  1 import tensorflow as tf
  2 from tflearn.layers.conv import global_avg_pool
  3 from tensorflow.contrib.layers import batch_norm, flatten
  4 from tensorflow.contrib.layers import xavier_initializer
  5 from tensorflow.contrib.framework import arg_scope
  6 
  7 class DenseNet():
  8     def __init__(self, x, n_classes, nb_blocks, filters, dropout_rate, training):
  9         self.nb_blocks = nb_blocks
 10         self.filters = filters
 11         self.training = training
 12         self.dropout_rate = dropout_rate
 13         self.n_classes = n_classes
 14         self.model = self.Dense_net(x)
 15 
 16     def bottleneck_layer(self, x, scope):
 17         # print(x)
 18         with tf.name_scope(scope):
 19             x = self._Batch_Normalization(x, training=self.training, scope=scope+'_batch1')
 20             x = self._Relu(x)
 21             x = self._conv_layer(x, filter=4 * self.filters, kernel=[1,1], layer_name=scope+'_conv1')
 22             x = self._Drop_out(x, rate=self.dropout_rate, training=self.training)
 23 
 24             x = self._Batch_Normalization(x, training=self.training, scope=scope+'_batch2')
 25             x = self._Relu(x)
 26             x = self._conv_layer(x, filter=self.filters, kernel=[3,3], layer_name=scope+'_conv2')
 27             x = self._Drop_out(x, rate=self.dropout_rate, training=self.training)
 28 
 29             # print(x)
 30 
 31             return x
 32 
 33     def transition_layer(self, x, scope):
 34         with tf.name_scope(scope):
 35             x = self._Batch_Normalization(x, training=self.training, scope=scope+'_batch1')
 36             x = self._Relu(x)
 37             x = self._conv_layer(x, filter=self.filters, kernel=[1,1], layer_name=scope+'_conv1')
 38             x = self._Drop_out(x, rate=self.dropout_rate, training=self.training)
 39             x = self._Average_pooling(x, pool_size=[2,2], stride=2)
 40 
 41             return x
 42 
 43     def dense_block(self, input_x, nb_layers, layer_name):
 44         with tf.name_scope(layer_name):
 45             layers_concat = list()
 46             layers_concat.append(input_x)
 47 
 48             x = self.bottleneck_layer(input_x, scope=layer_name + '_bottleN_' + str(0))
 49 
 50             layers_concat.append(x)
 51 
 52             for i in range(nb_layers - 1):
 53                 x = self._Concatenation(layers_concat)
 54                 x = self.bottleneck_layer(x, scope=layer_name + '_bottleN_' + str(i + 1))
 55                 layers_concat.append(x)
 56 
 57             x = self._Concatenation(layers_concat)
 58 
 59             return x
 60 
 61     def Dense_net(self, input_x):
 62         x = self._conv_layer(input_x, filter=2 * self.filters, kernel=[7,7], stride=2, layer_name='conv0')
 63         # x = Max_Pooling(x, pool_size=[3,3], stride=2)
 64 
 65 
 66         """
 67         for i in range(self.nb_blocks) :
 68             # 6 -> 12 -> 48
 69             x = self.dense_block(input_x=x, nb_layers=4, layer_name='dense_'+str(i))
 70             x = self.transition_layer(x, scope='trans_'+str(i))
 71         """
 72 
 73 
 74 
 75 
 76         x = self.dense_block(input_x=x, nb_layers=6, layer_name='dense_1')
 77         x = self.transition_layer(x, scope='trans_1')
 78 
 79         x = self.dense_block(input_x=x, nb_layers=12, layer_name='dense_2')
 80         x = self.transition_layer(x, scope='trans_2')
 81 
 82         x = self.dense_block(input_x=x, nb_layers=48, layer_name='dense_3')
 83         x = self.transition_layer(x, scope='trans_3')
 84 
 85         x = self.dense_block(input_x=x, nb_layers=32, layer_name='dense_final')
 86 
 87         # 100 Layer
 88         x = self._Batch_Normalization(x, training=self.training, scope='linear_batch')
 89         x = self._Relu(x)
 90         x = self._Global_Average_Pooling(x)
 91         x = flatten(x)
 92         x = self._Linear(x)
 93 
 94 
 95         # x = tf.reshape(x, [-1, 10])
 96         return x
 97 
 98 
 99     def _conv_layer(self,input, filter, kernel, stride=1, layer_name="conv"):
100         with tf.name_scope(layer_name):
101             network = tf.layers.conv2d(inputs=input, use_bias=False, filters=filter, kernel_size=kernel, strides=st    ride, padding='SAME')
102         return network
103 
104 
105     def _Global_Average_Pooling(self, x, stride=1):
106         """
107         width = np.shape(x)[1]
108         height = np.shape(x)[2]
109         pool_size = [width, height]
110         return tf.layers.average_pooling2d(inputs=x, pool_size=pool_size, strides=stride) # The stride value does n    ot matter
111         It is global average pooling without tflearn
112         """
113 
114         return global_avg_pool(x, name='Global_avg_pooling')
115         # But maybe you need to install h5py and curses or not
116 
117 
118     def _Batch_Normalization(self, x, training, scope):
119         with arg_scope([batch_norm],
120                        scope=scope,
121                        updates_collections=None,
122                        decay=0.9,
123                        center=True,
124                        scale=True,
125                        zero_debias_moving_mean=True) :
126             return tf.cond(training,
127                            lambda : batch_norm(inputs=x, is_training=training, reuse=None),
128                            lambda : batch_norm(inputs=x, is_training=training, reuse=True))
129 
130     def _Drop_out(self, x, rate, training) :
131         return tf.layers.dropout(inputs=x, rate=rate, training=training)
132 
133     def _Relu(self, x):
134         return tf.nn.relu(x)
135 
136     def _Average_pooling(self, x, pool_size=[2,2], stride=2, padding='VALID'):
137         return tf.layers.average_pooling2d(inputs=x, pool_size=pool_size, strides=stride, padding=padding)
138 
139 
140     def _Max_Pooling(self, x, pool_size=[3,3], stride=2, padding='VALID'):
141         return tf.layers.max_pooling2d(inputs=x, pool_size=pool_size, strides=stride, padding=padding)
142 
143     def _Concatenation(self,layers) :
144         return tf.concat(layers, axis=3)
145 
146     def _Linear(self,x) :
147         return tf.layers.dense(inputs=x, units=self.n_classes, name='linear')
```

